{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862ebc84-0f1e-405a-b7b9-f9de92591466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/app/few_shot_unlearning/FSL/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from dassl.config import get_cfg_default\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim \n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from dassl.data.datasets.build import build_dataset\n",
    "from dassl.data.transforms.transforms import build_transform\n",
    "from dassl.data.data_manager import build_data_loader\n",
    "\n",
    "import FSL.datasets.stanford_cars\n",
    "import FSL.datasets.stanford_dogs\n",
    "import FSL.datasets.caltech101\n",
    "import FSL.datasets.oxford_flowers\n",
    "import FSL.datasets.oxford_pets\n",
    "import FSL.datasets.food101\n",
    "import FSL.datasets.eurosat\n",
    "import FSL.datasets.sun397\n",
    "import FSL.datasets.fgvc_aircraft\n",
    "import FSL.datasets.cub\n",
    "import FSL.datasets.ucf101\n",
    "import FSL.datasets.plantdoc\n",
    "import FSL.datasets.imagenet\n",
    "\n",
    "from utils.eval_utils import *\n",
    "import utils.ssd as ssd\n",
    "\n",
    "\n",
    "all_ds = ['StanfordDogs', 'StanfordCars',  'Caltech101', 'OxfordFlowers', 'Food101', 'DescribableTextures', 'EuroSAT', 'SUN397', 'FGVCAircraft', 'CUB', 'UCF101', 'PLANTDOC']\n",
    "val_ds = [ 'EuroSAT', 'SUN397','Food101', 'DescribableTextures','PLANTDOC']\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "cfg = get_cfg_default()\n",
    "\n",
    "cfg.merge_from_file(\"configs/trainers/mainconfig/adam_lr2e-4_B256_ep200_ViT16.yaml\")\n",
    "\n",
    "cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"\n",
    "cfg.SEED = 0\n",
    "cfg.DATASET.ROOT = \"/app/datasets/\"\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "# cfg.USE_CUDA = False\n",
    "cfg.DATASET.NUM_SHOTS = -1\n",
    "\n",
    "cfg.DATALOADER.TRAIN_X.BATCH_SIZE = 4\n",
    "cfg.DATALOADER.TEST.BATCH_SIZE = 16\n",
    "\n",
    "backbone_arch = \"ViT-B/16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8452e6d-c7fe-4d0d-a326-8ce1bef731f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503eae62-dd81-43ad-af84-d9b1007224a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b91625-d52d-4e40-b81c-01a9be7f7dec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: StanfordDogs\n",
      "Reading split from /app/datasets/stanford_dogs/split_alexey_stanford_dogs.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: StanfordCars\n",
      "Reading split from /app/datasets/stanford_cars/split_zhou_StanfordCars.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: Caltech101\n",
      "Reading split from /app/datasets/caltech-101/split_zhou_Caltech101.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /app/datasets/oxford_flowers/split_zhou_OxfordFlowers.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: Food101\n",
      "Reading split from /app/datasets/food-101/split_zhou_Food101.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: DescribableTextures\n",
      "Reading split from /app/datasets/dtd/split_zhou_DescribableTextures.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /app/datasets/eurosat/split_zhou_EuroSAT.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: SUN397\n",
      "Reading split from /app/datasets/sun397/split_zhou_SUN397.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: FGVCAircraft\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: CUB\n",
      "Reading split from /app/datasets/cub/split_alexey_cub.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: UCF101\n",
      "Reading split from /app/datasets/ucf101/split_zhou_UCF101.json\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Loading dataset: PLANTDOC\n",
      "Reading split from /app/datasets/plantdoc/split_alexey_plantdoc.json\n"
     ]
    }
   ],
   "source": [
    "test_datasets = {}\n",
    "test_dataloaders = {}\n",
    "train_loaders = {}\n",
    "datasets_cls = {}\n",
    "classnames = {}\n",
    "\n",
    "cfg_original = copy.deepcopy(cfg)\n",
    "for ds in all_ds:\n",
    "    cfg = copy.deepcopy(cfg_original)\n",
    "    cfg.DATASET.NAME = ds\n",
    "    tfm_train = build_transform(cfg, is_train=True)\n",
    "    tfm_test = build_transform(cfg, is_train=False)\n",
    "\n",
    "    dataset = build_dataset(cfg)\n",
    "    test_loader_all = build_data_loader(\n",
    "                cfg,\n",
    "                sampler_type=cfg.DATALOADER.TEST.SAMPLER,\n",
    "                data_source=dataset.test,\n",
    "                batch_size=cfg.DATALOADER.TEST.BATCH_SIZE,\n",
    "                tfm=tfm_test,\n",
    "                is_train=False,\n",
    "                dataset_wrapper=None\n",
    "            )\n",
    "\n",
    "    train_loader_all = build_data_loader(\n",
    "                cfg,\n",
    "                sampler_type='RandomSampler',\n",
    "                data_source=dataset.train_x,\n",
    "                batch_size=64,\n",
    "                tfm = tfm_test,\n",
    "                is_train=False,\n",
    "                dataset_wrapper=None\n",
    "            )\n",
    "\n",
    "    test_datasets[ds] = dataset\n",
    "    test_dataloaders[ds] = test_loader_all\n",
    "    train_loaders[ds] = train_loader_all\n",
    "    datasets_cls[ds] = dataset\n",
    "    classnames[ds] = dataset.classnames\n",
    "\n",
    "\n",
    "with open(f\"assets/results_zs_all_ViT16.pkl\", \"rb\") as f:\n",
    "    results_zs = pickle.load(f)  \n",
    "\n",
    "myseed=cfg.SEED\n",
    "torch.manual_seed(myseed)\n",
    "random.seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be70b81-f045-40f8-b10b-2b172d3d7264",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552d7e85-f730-42c0-91d4-12b045aa72ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example of forgetting with SSD on StanfordDogs dataset\n",
    "##### Compute importances for SSD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81ac09e-ca3c-4ee2-9b14-cab0d73fd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed importances for all datasets\n",
    "\n",
    "dampening_constant = 1.\n",
    "selection_weighting = 30.\n",
    "parameters = {\n",
    "        \"lower_bound\": 1.,  # 1\n",
    "        \"exponent\": 1.,  # unused\n",
    "        \"magnitude_diff\": None,  # unused\n",
    "        \"min_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"max_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"forget_threshold\": 1,  # unused\n",
    "        \"dampening_constant\": dampening_constant,  # Lambda from paper\n",
    "        \"selection_weighting\": selection_weighting,  # Alpha from paper\n",
    "        \"batch_size\" : 64 # Important for importance calculations as quite sensitive!\n",
    "    }\n",
    "\n",
    "retain_loader = {\n",
    "                     'StanfordDogs' : train_loaders['StanfordDogs'], \n",
    "                     'StanfordCars': train_loaders['StanfordCars'], \n",
    "                     'Caltech101': train_loaders['Caltech101'], \n",
    "                     'OxfordFlowers' : train_loaders['OxfordFlowers'], \n",
    "    \n",
    "                     'CUB': train_loaders['CUB'], \n",
    "                     'UCF101' : train_loaders['UCF101'], \n",
    "    \n",
    "                     'FGVCAircraft': train_loaders['FGVCAircraft'], \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74ec102-025e-425a-b849-86ea9731f9b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..\n",
      "{'lower_bound': 1.0, 'exponent': 1.0, 'magnitude_diff': None, 'min_layer': -1, 'max_layer': -1, 'forget_threshold': 1, 'dampening_constant': 1.0, 'selection_weighting': 30.0, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "model = get_model(device=device, arch=backbone_arch)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "pdr = ssd.ParameterPerturber(model, optimizer, device, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2adb0-8d1c-433c-8a3a-8c23d453749e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_importances = pdr.calc_importance(retain_loader, classnames, aggregate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462904c-a84b-4342-a843-bac44dbff4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_all = {}\n",
    "for file in os.listdir(\"ssd_importances/\"):\n",
    "    if file.startswith(\"importance\"):\n",
    "        ds = torch.load(f\"ssd_importances/{file}\")\n",
    "        ds_name = file.split(\"_\")[1].replace(\".pt\", \"\")\n",
    "        print(ds_name)\n",
    "        importances_all[ds_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684cb77-1148-4a54-802e-267bf1d37e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(importances_all, f\"ssd_importances/all_importances_batch_64.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6a0f0-e95f-4ece-8be7-5c29c8074e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97d8e8b1-79b8-4044-b4a6-7d02ea61c5c3",
   "metadata": {},
   "source": [
    "##### Unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f095c9-60ec-426b-bdea-c5260bb2309d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forget_StanfordDogs_retain_Caltech101|OxfordFlowers|CUB_attempt_0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_ds = 'StanfordDogs'\n",
    "id_test = 0\n",
    "forget_loader = {forget_ds : train_loaders[forget_ds]}\n",
    "\n",
    "retain_loader = {\n",
    "                     'Caltech101': train_loaders['Caltech101'], \n",
    "                     'OxfordFlowers' : train_loaders['OxfordFlowers'], \n",
    "                     'CUB': train_loaders['CUB'],     \n",
    "                }\n",
    "\n",
    "retain_ds = '|'.join([k for k in retain_loader.keys()])\n",
    "                 \n",
    "full_name = f\"forget_{forget_ds}_retain_{retain_ds}_attempt_{id_test}\"\n",
    "\n",
    "forget_list = list(forget_loader.keys())\n",
    "retain_list = list(retain_loader.keys())\n",
    "    \n",
    "full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65ec93-8e25-475b-99ae-70215354f61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc59dc6-a500-46da-b485-49aff53e6ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len n_ds 1.0\n",
      "len n_ds 3.0\n"
     ]
    }
   ],
   "source": [
    "# path = f\"ssd_importances/all_importances_batch_64.pt\"\n",
    "path = f\"/app/few_shot_unlearning_old/ssd_importances/all_importances_batch_64.pt\"\n",
    "# Calculation of the forget set importances\n",
    "sample_importances = pdr.calc_importance_loaded(path, forget_list)\n",
    "\n",
    "# Calculate the importances of the retain sets\n",
    "original_importances = pdr.calc_importance_loaded(path, retain_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74f726a-a775-42b5-bc8b-150a9757a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..\n",
      "{'lower_bound': 1.0, 'exponent': 1.0, 'magnitude_diff': None, 'min_layer': -1, 'max_layer': -1, 'forget_threshold': 1, 'dampening_constant': 1.0, 'selection_weighting': 30.0, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "\n",
    "dampening_constant = 1.\n",
    "selection_weighting = 30.\n",
    "parameters = {\n",
    "        \"lower_bound\": 1.,  # 1\n",
    "        \"exponent\": 1.,  # unused\n",
    "        \"magnitude_diff\": None,  # unused\n",
    "        \"min_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"max_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"forget_threshold\": 1,  # unused\n",
    "        \"dampening_constant\": dampening_constant,  # Lambda from paper\n",
    "        \"selection_weighting\": selection_weighting,  # Alpha from paper\n",
    "        \"batch_size\" : 64 # Important for importance calculations as quite sensitive!\n",
    "    }\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "model = get_model(device=device, arch=backbone_arch)\n",
    "model = model.eval()\n",
    "\n",
    "pdr = ssd.ParameterPerturber(model, optimizer, device, parameters)\n",
    "\n",
    "# Dampen selected parameters\n",
    "pdr.modify_weight(original_importances, sample_importances, ignore_params=['logit_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0603d-da86-4876-bda5-ab6ee4263b1a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 521/521 [00:56<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ StanfordDogs - 0.020393474088291747 ++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 503/503 [01:21<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ StanfordCars - 0.6189528665588857 ++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 155/155 [00:12<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Caltech101 - 0.9217038539553752 ++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 2/154 [00:00<00:19,  7.83it/s]"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    results_ds = eval_all_ds(model, datasets_cls, test_dataloaders, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8bfd9-2cdb-425f-a9eb-12e171152eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5398a-e82d-4847-8501-9948bf243c0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1425a9e7-bef4-46c0-a5dd-52c193f661ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute MMD weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acf1ae-d1db-4a9a-ba31-3271d030611a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model(device=device, arch=backbone_arch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262bc0d-44a3-40ba-8d59-e60f7cbb6314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b836136-6697-48db-b72b-c96d04bb06e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "with torch.no_grad():\n",
    "    for key in train_loaders:\n",
    "        if key != 'ImageNet': continue\n",
    "        print(key)\n",
    "        features = {'text' : \"\", 'images' : []}\n",
    "        ds_loader = train_loaders[key]\n",
    "        for batch in tqdm(ds_loader):\n",
    "            img = batch['img'].cuda()\n",
    "            features['images'].append(model.encode_image(img).detach().cpu().numpy().squeeze())\n",
    "\n",
    "        features['images'] = np.concatenate(features['images'])\n",
    "        features['text'] = model.encode_text(clip.tokenize(datasets_cls[key].classnames).cuda())#.detach().cpu().numpy()\n",
    "\n",
    "        with open(f\"features_embeddings/features_{key}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0511506-1cad-42db-8487-8c8800fa375d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmd_similarities = {}\n",
    "\n",
    "main_path = f\"features_embeddings/features\"\n",
    "\n",
    "for key1 in tqdm(train_loaders):\n",
    "    for key2 in train_loaders:\n",
    "        key_ds = '_'.join(sorted([key1, key2]))\n",
    "        if key1 != key2 and key_ds not in mmd_similarities:\n",
    "            with open(f\"{main_path}_{key1}.pkl\", \"rb\") as f:\n",
    "                feat_key1 = pickle.load(f)\n",
    "            with open(f\"{main_path}_{key2}.pkl\", \"rb\") as f:\n",
    "                feat_key2 = pickle.load(f)\n",
    "                \n",
    "            mmd_similarities[key_ds] = mmd_rbf(feat_key1['images'], feat_key2['images'], gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e9eae-91ec-4133-85e1-608d0f547947",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_similarities_text = {}\n",
    "\n",
    "for key1 in tqdm(train_loaders):\n",
    "    for key2 in train_loaders:\n",
    "        key_ds = '_'.join(sorted([key1, key2]))\n",
    "        if key1 != key2 and key_ds not in mmd_similarities_text:\n",
    "            with open(f\"{main_path}_{key1}.pkl\", \"rb\") as f:\n",
    "                feat_key1 = pickle.load(f)\n",
    "            with open(f\"{main_path}_{key2}.pkl\", \"rb\") as f:\n",
    "                feat_key2 = pickle.load(f)\n",
    "                \n",
    "            mmd_similarities_text[key_ds] = mmd_rbf(feat_key1['text'].detach().cpu().numpy(), feat_key2['text'].detach().cpu().numpy(), gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29023690-159a-4c61-8824-3824dcc75469",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"features_embeddings/mmd_sim_images.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mmd_similarities, f)\n",
    "    \n",
    "with open(f\"features_embeddings/mmd_sim_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mmd_similarities_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f874b2-921b-4977-9354-fdee8726a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bc2f9-7c33-452c-b584-a672baa956b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"features_embeddings/mmd_sim_images.pkl\", \"rb\") as f:\n",
    "    mmd_similarities = pickle.load(f)\n",
    "    \n",
    "with open(f\"features_embeddings/mmd_sim_text.pkl\", \"rb\") as f:\n",
    "    mmd_similarities_text = pickle.load(f)\n",
    "\n",
    "zs_clip_results =  {\n",
    "    \"EuroSAT\": 48.383,\n",
    "    \"StanfordCars\": 65.514,\n",
    "    \"PLANTDOC\": 34.994,\n",
    "    \"DescribableTextures\": 43.972,\n",
    "    \"StanfordDogs\": 59.117,\n",
    "    \"SUN397\": 62.579,\n",
    "    \"FGVCAircraft\": 24.752,\n",
    "    \"CUB\": 55.009,\n",
    "    \"Caltech101\": 93.306,\n",
    "    \"Food101\": 85.888,\n",
    "    \"UCF101\": 67.46,\n",
    "    \"OxfordFlowers\": 70.767\n",
    "}\n",
    "\n",
    "weights = weighted_loss(forget_ds, val_ds, mmd_similarities_text, mmd_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca6e1d-4736-41a1-91cc-5b7df8284e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3b56b-9c1e-4d43-a3f1-5fb8b909a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = {}\n",
    "for k in zs_clip_results:\n",
    "    if k != forget_ds:\n",
    "        print(k, round(max(zs_clip_results[k] - results_ds[k]['all']['all_ds']*100, 0), 3))\n",
    "        diff[k] = zs_clip_results[k] - results_ds[k]['all']['all_ds']*100\n",
    "        \n",
    "# difference on validation sets (list knowledge of CLIP)\n",
    "np.sum([diff[k] * weights[k] for k in val_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b2b7e-a877-4765-94cf-bffde90ea938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a240f-7119-46c2-a0d6-9aa326beaff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
